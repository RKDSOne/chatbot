{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot using RNN\n",
    "### Northwestern University - Fall 2017\n",
    "### Student: Danilo Neves Ribeiro\n",
    "### E-mail: daniloribeiro2021@u.northwestern.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The idea of the project was to train a simple chatbot using Recurrent Neural Networks. \n",
    "\n",
    "## Chatbots\n",
    "There are many ways one can go about creating a chat-bot. For example, many chatbots rely on pre-defined rules to answer questions. Those can work well but requires intese human work to create as many rules as possible. \n",
    "\n",
    "Machine learning greately simplify this task by enableing to learn from pre-existing conversation corpus. The two main types of ML chatbots are:\n",
    "\n",
    "- Retrieval-based: answer questions by choosing from one of the answers available in the data-set.\n",
    "- Generative: generates the conversation dialog word by word based on the query. The generated sentense is normally not included in the original data-set.\n",
    "\n",
    "For this project, I decided to create a chatbot using the generative approch, which normally makes more mistakes, such as grammar mistakes, but can respond a broader set of questions and contexts.\n",
    "\n",
    "## Dataset\n",
    "The model was trained using the [Cornell Movie Dialog Corpus](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html), that contains a collection of fictional conversations extracted from raw movie scripts. \n",
    "\n",
    "## Implementation Architecture\n",
    "Here I use a RNN to train on the data set. More specifically I use a seq2seq model with bucketing and attention mechanism, which is described in more details below:\n",
    "\n",
    "### Seq2Seq:\n",
    "Sequence to Sequence RNN models are composed of two main components: encoder and decoder. The encoder is responsible for reading the input, word by word, and generating a hidden state that \"represents\" the input. The decoder outputs words according to the hidden states generated by the encoder. The following image gives a general idea of this architecture:\n",
    "<img src=\"seq2seq.png\" alt=\"seq2seq\" style=\"width: 700px;\"/>\n",
    "\n",
    "### Padding and Bucketing: \n",
    "One of the limitations of the simple Seq2Seq arquitectures is that it has fixed size input and output. Therefore we need to use padding and special symbols to deal with the fact that both input and output sentences can have different length (the ones used here are: EOS = \"End of sentence\", PAD = \"Filler\", GO = \"Start decoding\", plus a special symbol for unknown words: UNK).\n",
    "\n",
    "To efficiently handle sentenses with different lengths we use the bucketing method. Our model uses buckets = [(5, 10), (10, 15), (20, 25), (40, 50)], this means that if the input is a sentence with 3 tokens, and the corresponding output is a sentence with 6 tokens, then they will be put in the first bucket and padded to length 5 for encoder inputs, and length 10 for decoder inputs.\n",
    "\n",
    "### Attention mechanism:\n",
    "The attention mechanism tries to address the following limitations:\n",
    "- The decoder is not aware of which parts of the encoding are relevant at each step of the generation.\n",
    "- The encoder has limited memory and can't \"remember\" more than a single fixed size vector.\n",
    "\n",
    "The attention model comes between the encoder and the decoder and helps the decoder to pick only the encoded inputs that ar important for each step of the decoding process:\n",
    "\n",
    "<img src=\"attention.jpg\" alt=\"Attention mechanism\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "\n",
    "The code will be split between: \n",
    "- Preprocessing data (tokenizing, creating vobabulary, transforming input from words to word ids)\n",
    "- Training\n",
    "- Evaluation\n",
    "\n",
    "##### Software requirements\n",
    "- Python 3.6.2\n",
    "- Numpy\n",
    "- TensorFlow \n",
    "\n",
    "**_Note_**: this code is based on the code from the chatbot tutorial by [Suriyadeepan Ram](http://suriyadeepan.github.io/2016-06-28-easy-seq2seq/) and also uses the more general seq2seq model provided by the [Google Tensorflow tutorial on NMT](https://github.com/tensorflow/nmt), which is imported from a separate code file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from seq2seq_model import Seq2SeqModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GLOBAL VARIABLES AND PARAMS\n",
    "\n",
    "# encoding and decoding paths\n",
    "TRAIN_END_PATH = os.path.join('data', 'train.enc')\n",
    "TRAIN_DEC_PATH = os.path.join('data', 'train.dec')\n",
    "TEST_END_PATH = os.path.join('data', 'test.enc')\n",
    "TEST_DEC_PATH = os.path.join('data', 'test.dec')\n",
    "\n",
    "TRAIN_END_ID_PATH = os.path.join('data', 'train.enc.id')\n",
    "TRAIN_DEC_ID_PATH = os.path.join('data', 'train.dec.id')\n",
    "TEST_END_ID_PATH = os.path.join('data', 'test.enc.id')\n",
    "TEST_DEC_ID_PATH = os.path.join('data', 'test.dec.id')\n",
    "\n",
    "# vocabulary paths\n",
    "VOCAB_ENC_PATH = os.path.join('data', 'vocab.enc')\n",
    "VOCAB_DEC_PATH = os.path.join('data', 'vocab.dec')\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "\n",
    "# data utils\n",
    "SPLIT_REGEX = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "PAD_TOKEN = \"_PAD\"\n",
    "START_TOKEN = \"_GO\"\n",
    "END_TOKEN = \"_EOS\"\n",
    "UNKNOWEN_TOKEN = \"_UNK\"\n",
    "INIT_VOCAB = [PAD_TOKEN, START_TOKEN, END_TOKEN, UNKNOWEN_TOKEN]\n",
    "\n",
    "# args\n",
    "BUCKETS = [(5, 10), (10, 15), (20, 25), (40, 50)]\n",
    "LSTM_LAYES = 3\n",
    "LAYER_SIZE = 256\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.5\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.99\n",
    "MAX_GRADIENT_NORM = 5.0\n",
    "\n",
    "# pre training\n",
    "TRAINED_MODEL_PATH = 'pre_trained'\n",
    "TRAINED_VOCAB_ENC = os.path.join('pre_trained', 'vocab.enc')\n",
    "TRAINED_VOCAB_DEC = os.path.join('pre_trained', 'vocab.dec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SIMPLE TOKENIZER\n",
    "\n",
    "def tokenize(sentense):\n",
    "    tokens = []\n",
    "    for token in sentense.strip().split():\n",
    "        tokens.extend([x for x in re.split(SPLIT_REGEX, token) if x])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CREATING VOCABULARY\n",
    "\n",
    "def create_vocab(data_path, vocab_path):\n",
    "    vocab = {}    \n",
    "    # only creates new file if file doesn't exist\n",
    "    if os.path.exists(vocab_path):\n",
    "        print(\"file \", vocab_path, \" already exists\") \n",
    "    else:\n",
    "        with open(data_path, 'r') as data_file:\n",
    "            for line in data_file:\n",
    "                tokens = tokenize(line)\n",
    "                for token in tokens:\n",
    "                    if token not in vocab:\n",
    "                        vocab[token] = 1\n",
    "                    else:\n",
    "                        vocab[token] += 1\n",
    "        # use the default tokens as initial vocabulity words\n",
    "        vocab_list = INIT_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "        # trim vocabulary\n",
    "        vocab_list = vocab_list[:MAX_VOCAB_SIZE]\n",
    "        print(\"final vacabulary size for \", data_path, \" = \", len(vocab_list))\n",
    "        # save to file\n",
    "        with open(vocab_path, 'w') as vocab_file:\n",
    "            for word in vocab_list:\n",
    "                vocab_file.write(word + \"\\n\")   \n",
    "        # update vocab with new order\n",
    "        vocab = dict([(y, x) for (x, y) in enumerate(vocab_list)])\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRANSFORM WORDS IN DATA TO IDS\n",
    "\n",
    "def from_text_data_to_id_list(data_path, ouput_path, vocab):\n",
    "    # only creates new file is file doesn't exist\n",
    "    if os.path.exists(ouput_path):\n",
    "        print(\"file \", ouput_path, \" already exists\") \n",
    "    else:\n",
    "        with open(data_path, 'r') as data_file:\n",
    "            with open(ouput_path, 'w') as ouput_file:\n",
    "                for line in data_file:\n",
    "                    tokens = tokenize(line)\n",
    "                    id_list = [str(vocab.get(word, vocab.get(UNKNOWEN_TOKEN))) for word in tokens]\n",
    "                    ouput_file.write(\" \".join(id_list) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREPROCESSING\n",
    "\n",
    "def preprocess_data():\n",
    "    encoding_vocab = create_vocab(TRAIN_END_PATH, VOCAB_ENC_PATH)\n",
    "    decoding_vocab = create_vocab(TRAIN_DEC_PATH, VOCAB_DEC_PATH)\n",
    "    from_text_data_to_id_list(TRAIN_END_PATH, TRAIN_END_ID_PATH, encoding_vocab)\n",
    "    from_text_data_to_id_list(TRAIN_DEC_PATH, TRAIN_DEC_ID_PATH, decoding_vocab)\n",
    "    from_text_data_to_id_list(TEST_END_PATH, TEST_END_ID_PATH, encoding_vocab)\n",
    "    from_text_data_to_id_list(TEST_DEC_PATH, TEST_DEC_ID_PATH, decoding_vocab)\n",
    "    print(\"Data preprocessing complete.\")\n",
    "\n",
    "preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(source_path, target_path):\n",
    "    data_set = [[] for _ in BUCKETS]\n",
    "    with tf.gfile.GFile(source_path, mode=\"r\") as source_file:\n",
    "        with tf.gfile.GFile(target_path, mode=\"r\") as target_file:\n",
    "            source, target = source_file.readline(), target_file.readline()\n",
    "            while source and target:\n",
    "                source_ids = [int(x) for x in source.split()]\n",
    "                target_ids = [int(x) for x in target.split()]\n",
    "                target_ids.append(INIT_VOCAB.index(END_TOKEN))\n",
    "                for bucket_id, (source_size, target_size) in enumerate(BUCKETS):\n",
    "                    if len(source_ids) < source_size and len(target_ids) < target_size:\n",
    "                        data_set[bucket_id].append([source_ids, target_ids])\n",
    "                        break\n",
    "                source, target = source_file.readline(), target_file.readline()\n",
    "                return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CREATE MODEL\n",
    "\n",
    "def create_model(forward_only):\n",
    "    # TODO: remove\n",
    "    print(MAX_VOCAB_SIZE, \"\\n\", MAX_VOCAB_SIZE, \"\\n\", BUCKETS, \"\\n\", LAYER_SIZE, \"\\n\", LSTM_LAYES, \"\\n\", MAX_GRADIENT_NORM, \"\\n\", \n",
    "        BATCH_SIZE, \"\\n\", LEARNING_RATE, \"\\n\", LEARNING_RATE_DECAY_FACTOR, \"\\n\", forward_only, \"\\n\")\n",
    "    return Seq2SeqModel(\n",
    "        MAX_VOCAB_SIZE, MAX_VOCAB_SIZE, BUCKETS, LAYER_SIZE, LSTM_LAYES, MAX_GRADIENT_NORM, \n",
    "        BATCH_SIZE, LEARNING_RATE, LEARNING_RATE_DECAY_FACTOR, forward_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAIN MODEL\n",
    "\n",
    "def train():\n",
    "    # prepare dataset\n",
    "    enc_train, dec_train, enc_dev, dec_dev\n",
    "\n",
    "    # setup config to use BFC allocator\n",
    "    config = tf.ConfigProto()    \n",
    "    with tf.Session(config=config) as sess:\n",
    "        # Create model.\n",
    "        model = create_model(forward_only = False)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Read data into buckets and compute their sizes.\n",
    "        dev_set = read_data(enc_dev, dec_dev)\n",
    "        train_set = read_data(enc_train, dec_train)\n",
    "        train_bucket_sizes = [len(train_set[b]) for b in xrange(len(_buckets))]\n",
    "        train_total_size = float(sum(train_bucket_sizes))\n",
    "\n",
    "        # A bucket scale is a list of increasing numbers from 0 to 1 that we'll use\n",
    "        # to select a bucket. Length of [scale[i], scale[i+1]] is proportional to\n",
    "        # the size if i-th training bucket, as used later.\n",
    "        train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                               for i in xrange(len(train_bucket_sizes))]\n",
    "\n",
    "        # This is the training loop.\n",
    "        step_time, loss = 0.0, 0.0\n",
    "        current_step = 0\n",
    "        while True:\n",
    "            # Choose a bucket according to data distribution. We pick a random number\n",
    "            # in [0, 1] and use the corresponding interval in train_buckets_scale.\n",
    "            random_number_01 = np.random.random_sample()\n",
    "            bucket_id = min([i for i in xrange(len(train_buckets_scale))\n",
    "                           if train_buckets_scale[i] > random_number_01])\n",
    "\n",
    "            # Get a batch and make a step.\n",
    "            encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "              train_set, bucket_id)\n",
    "            _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                       target_weights, bucket_id, False)            \n",
    "            loss += step_loss / gConfig['steps_per_checkpoint']\n",
    "            current_step += 1\n",
    "\n",
    "            # Once in a while, we save checkpoint, print statistics, and run evals.\n",
    "            if current_step % gConfig['steps_per_checkpoint'] == 0:\n",
    "                # Print statistics for the previous epoch.\n",
    "                perplexity = math.exp(loss) if loss < 300 else float('inf')\n",
    "                print (\"global step %d learning rate %.4f step-time %.2f perplexity \"\n",
    "                       \"%.2f\" % (model.global_step.eval(), model.learning_rate.eval(),\n",
    "                                 step_time, perplexity))\n",
    "                sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD PRE-TRAINED MODEL\n",
    "\n",
    "def load_vocabulary_list(vocabulary_path):\n",
    "    with open(vocabulary_path, mode=\"r\") as vocab_file:\n",
    "        return [line.strip() for line in vocab_file.readlines()]\n",
    "\n",
    "def load_pre_trained_model(session):\n",
    "    print(\"Loading vocab...\")\n",
    "    enc_vocab_list = load_vocabulary_list(TRAINED_VOCAB_ENC)\n",
    "    dec_vocab_list = load_vocabulary_list(TRAINED_VOCAB_DEC)\n",
    "    enc_vocab = dict([(x, y) for (y, x) in enumerate(dec_vocab_list)])\n",
    "    rev_dec_vocab = dict(enumerate(dec_vocab_list))\n",
    "    \n",
    "    print(\"Creting model...\")\n",
    "    model = create_model(forward_only = True)\n",
    "\n",
    "    print(\"Loading saved model...\")\n",
    "    ckpt = tf.train.get_checkpoint_state(TRAINED_MODEL_PATH)\n",
    "    model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    return (model, enc_vocab, rev_dec_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DECODING\n",
    "\n",
    "def decode():\n",
    "    print(\"Start decoding...\")\n",
    "    with tf.Session() as sess:\n",
    "        model, enc_vocab, rev_dec_vocab = load_pre_trained_model(sess)\n",
    "        model.batch_size = 1  # We decode one sentence at a time.\n",
    "        \n",
    "        # Decode from standard input.\n",
    "        sys.stdout.write(\"> \")\n",
    "        sys.stdout.flush()\n",
    "        sentence = sys.stdin.readline()\n",
    "        while sentence:\n",
    "            # Get token-ids for the input sentence.\n",
    "            token_ids = [enc_vocab.get(w, INIT_VOCAB.index(UNKNOWEN_TOKEN)) for w in words]\n",
    "            # Which bucket does it belong to?\n",
    "            bucket_id = min([b for b in xrange(len(_buckets))\n",
    "                           if _buckets[b][0] > len(token_ids)])\n",
    "            # Get a 1-element batch to feed the sentence to the model.\n",
    "            encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "              {bucket_id: [(token_ids, [])]}, bucket_id)\n",
    "            # Get output logits for the sentence.\n",
    "            _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                           target_weights, bucket_id, True)\n",
    "            # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "            outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "            # If there is an EOS symbol in outputs, cut them at that point.\n",
    "            if data_utils.EOS_ID in outputs:\n",
    "                outputs = outputs[:outputs.index(data_utils.EOS_ID)]\n",
    "                # Print out French sentence corresponding to outputs.\n",
    "                print(\" \".join([tf.compat.as_str(rev_dec_vocab[output]) for output in outputs]))\n",
    "                print(\"> \", end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                sentence = sys.stdin.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start decoding...\n",
      "Loading vocab...\n",
      "Creting model...\n",
      "20000 \n",
      " 20000 \n",
      " [(5, 10), (10, 15), (20, 25), (40, 50)] \n",
      " 256 \n",
      " 3 \n",
      " 5.0 \n",
      " 64 \n",
      " 0.5 \n",
      " 0.99 \n",
      " True \n",
      "\n",
      "Loading saved model...\n",
      "INFO:tensorflow:Restoring parameters from pre_trained\\seq2seq.ckpt-21300\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Assign requires shapes of both tensors to match. lhs shape= [1536,256] rhs shape= [768,256]\n\t [[Node: save/Assign_7 = Assign[T=DT_FLOAT, _class=[\"loc:@embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/kernel\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/kernel, save/RestoreV2_7)]]\n\nCaused by op 'save/Assign_7', defined at:\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2808, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-e2fbaaba3ddd>\", line 1, in <module>\n    decode()\n  File \"<ipython-input-5-61b61ffcc40a>\", line 6, in decode\n    model, enc_vocab, rev_dec_vocab = load_pre_trained_model(sess)\n  File \"<ipython-input-4-b1c59e9d1fc6>\", line 15, in load_pre_trained_model\n    model = create_model(forward_only = True)\n  File \"<ipython-input-3-524b219ee88c>\", line 9, in create_model\n    BATCH_SIZE, LEARNING_RATE, LEARNING_RATE_DECAY_FACTOR, forward_only)\n  File \"C:\\Users\\dnr2\\Documents\\Northwestern\\Fall 2017\\EECS 495 - Deep learning foundations from scratch\\Project\\chatbot\\seq2seq_model.py\", line 175, in __init__\n    self.saver = tf.train.Saver(tf.global_variables())\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1218, in __init__\n    self.build()\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1227, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1263, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 751, in _build_internal\n    restore_sequentially, reshape)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 439, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 160, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 276, in assign\n    validate_shape=validate_shape)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 56, in assign\n    use_locking=use_locking, name=name)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [1536,256] rhs shape= [768,256]\n\t [[Node: save/Assign_7 = Assign[T=DT_FLOAT, _class=[\"loc:@embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/kernel\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/kernel, save/RestoreV2_7)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [1536,256] rhs shape= [768,256]\n\t [[Node: save/Assign_7 = Assign[T=DT_FLOAT, _class=[\"loc:@embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/kernel\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/kernel, save/RestoreV2_7)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e2fbaaba3ddd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-61b61ffcc40a>\u001b[0m in \u001b[0;36mdecode\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Start decoding...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrev_dec_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_pre_trained_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m  \u001b[1;31m# We decode one sentence at a time.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-b1c59e9d1fc6>\u001b[0m in \u001b[0;36mload_pre_trained_model\u001b[1;34m(session)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loading saved model...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mckpt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_checkpoint_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAINED_MODEL_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrev_dec_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1664\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1665\u001b[0m       sess.run(self.saver_def.restore_op_name,\n\u001b[1;32m-> 1666\u001b[1;33m                {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[0;32m   1667\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1668\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_eager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1336\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [1536,256] rhs shape= [768,256]\n\t [[Node: save/Assign_7 = Assign[T=DT_FLOAT, _class=[\"loc:@embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/kernel\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/kernel, save/RestoreV2_7)]]\n\nCaused by op 'save/Assign_7', defined at:\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2808, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-e2fbaaba3ddd>\", line 1, in <module>\n    decode()\n  File \"<ipython-input-5-61b61ffcc40a>\", line 6, in decode\n    model, enc_vocab, rev_dec_vocab = load_pre_trained_model(sess)\n  File \"<ipython-input-4-b1c59e9d1fc6>\", line 15, in load_pre_trained_model\n    model = create_model(forward_only = True)\n  File \"<ipython-input-3-524b219ee88c>\", line 9, in create_model\n    BATCH_SIZE, LEARNING_RATE, LEARNING_RATE_DECAY_FACTOR, forward_only)\n  File \"C:\\Users\\dnr2\\Documents\\Northwestern\\Fall 2017\\EECS 495 - Deep learning foundations from scratch\\Project\\chatbot\\seq2seq_model.py\", line 175, in __init__\n    self.saver = tf.train.Saver(tf.global_variables())\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1218, in __init__\n    self.build()\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1227, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1263, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 751, in _build_internal\n    restore_sequentially, reshape)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 439, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 160, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 276, in assign\n    validate_shape=validate_shape)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 56, in assign\n    use_locking=use_locking, name=name)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\dnr2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [1536,256] rhs shape= [768,256]\n\t [[Node: save/Assign_7 = Assign[T=DT_FLOAT, _class=[\"loc:@embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/kernel\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/kernel, save/RestoreV2_7)]]\n"
     ]
    }
   ],
   "source": [
    "decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "\n",
    "## Project Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here I present some of the challenges I faced when tring to train the model for this project:\n",
    "\n",
    "- Initially I tried to use the [ubuntu-dialog corpus](http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/). The dataset proved to be very large (a few Gb) and it took several hours just to preprocess the data. I decided that this corpus would be to complex to train on and decided to use the The Cornell Movie Dialog Corpus.\n",
    "- The Cornell Movie Dialog Corpus is a smaller and more manageble dataset, but training the model still took several hours (almost 2 entire days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
