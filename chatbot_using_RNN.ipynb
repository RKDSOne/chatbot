{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot using RNN\n",
    "### Northwestern University - Fall 2017\n",
    "### Student: Danilo Neves Ribeiro\n",
    "### E-mail: daniloribeiro2021@u.northwestern.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The idea of the project will be to implement a simple chat-bot using Recurrent neural networks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Architecture\n",
    "Here I use a RNN to train on the data set.\n",
    "\n",
    "## Software requirements\n",
    "- Python 3.6.2\n",
    "- Numpy\n",
    "- TensorFlow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import codecs\n",
    "from nmt import nmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARIABLES AND PARAMS\n",
    "\n",
    "# encoding and decoding\n",
    "TRAIN_END_PATH = os.path.join('data', 'train.enc')\n",
    "TRAIN_DEC_PATH = os.path.join('data', 'train.dec')\n",
    "TEST_END_PATH = os.path.join('data', 'test.enc')\n",
    "TEST_DEC_PATH = os.path.join('data', 'test.dec')\n",
    "\n",
    "# vocabulary\n",
    "VOCAB_ENC_PATH = os.path.join('data', 'vocab.enc')\n",
    "VOCAB_DEC_PATH = os.path.join('data', 'vocab.dec')\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "\n",
    "# data utils\n",
    "SPLIT_REGEX = re.compile(\"[\\w]+|[.,!?\\\"':;)(]\")\n",
    "UNKNOWEN_TOKEN = \"<unk>\"\n",
    "START_TOKEN = \"<s>\"\n",
    "END_TOKEN = \"</s>\"\n",
    "INIT_VOCAB = [UNKNOWEN_TOKEN, START_TOKEN, END_TOKEN]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZER\n",
    "\n",
    "def tokenize(sentense):\n",
    "    return re.findall(SPLIT_REGEX, sentense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CREATING VOCABULARY\n",
    "\n",
    "def create_vocab(data_path, vocab_path):\n",
    "    vocab = {}    \n",
    "    # only creates new file is file doesn't exist\n",
    "    if os.path.exists(vocab_path):\n",
    "       print(\"file \", vocab_path, \" already exists\") \n",
    "    else:\n",
    "        with open(data_path, 'r') as data_file:\n",
    "            for line in data_file:\n",
    "                tokens = tokenize(line)\n",
    "                for token in tokens:\n",
    "                    if token not in vocab:\n",
    "                        vocab[token] = 1\n",
    "                    else:\n",
    "                        vocab[token] += 1\n",
    "        # use the default tokens as initial vocabulity words\n",
    "        vocab_list = INIT_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "        # trim vocabulary\n",
    "        vocab_list = vocab_list[:MAX_VOCAB_SIZE]\n",
    "        print(\"final vacabulary size for \", data_path, \" = \", len(vocab_list))\n",
    "        # save to file\n",
    "        with codecs.open(vocab_path, 'w', encoding='utf-8') as vocab_file:\n",
    "            for word in vocab_list:\n",
    "                vocab_file.write(word + \"\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final vacabulary size for  data\\train.enc  =  20000\n",
      "final vacabulary size for  data\\train.dec  =  20000\n"
     ]
    }
   ],
   "source": [
    "# PREPARING DATA\n",
    "\n",
    "def prepare_data():\n",
    "        create_vocab(TRAIN_END_PATH, VOCAB_ENC_PATH)\n",
    "        create_vocab(TRAIN_DEC_PATH, VOCAB_DEC_PATH)\n",
    "        \n",
    "prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "def evaluate():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# interface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "\n",
    "## Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
